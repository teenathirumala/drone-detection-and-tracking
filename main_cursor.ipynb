{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"dataset\"\n",
    "OUTPUT_DIR = \"processed_dataset\"\n",
    "TARGET_SIZE = (640, 360)  # Width, Height\n",
    "DRONE_BOX_SIZE = 20       # Bounding box size in original resolution\n",
    "FRAME_NAME_PREFIX = \"frame\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{OUTPUT_DIR}/images\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/labels\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_pair(video_path, txt_path, global_frame_counter):\n",
    "    # Read coordinate data from txt file\n",
    "    coord_data = {}\n",
    "    with open(txt_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            frame_num = int(float(parts[0]))  # Handle 1.000000 format\n",
    "            x, y = map(float, parts[1:3])\n",
    "            coord_data[frame_num] = (x, y)\n",
    "\n",
    "    # Process video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Calculate scaling factors\n",
    "    orig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    orig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    scale_x = TARGET_SIZE[0] / orig_width\n",
    "    scale_y = TARGET_SIZE[1] / orig_height\n",
    "\n",
    "    with tqdm(total=total_frames, desc=os.path.basename(video_path)) as pbar:\n",
    "        video_frame_counter = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            video_frame_counter += 1  # Video frames are 1-indexed\n",
    "\n",
    "            # Resize frame\n",
    "            resized_frame = cv2.resize(frame, TARGET_SIZE)\n",
    "\n",
    "            # Get coordinates for this frame (0,0 means no drone)\n",
    "            x, y = coord_data.get(video_frame_counter, (0, 0))\n",
    "\n",
    "            # Save frame with global numbering\n",
    "            frame_filename = f\"{FRAME_NAME_PREFIX}_{global_frame_counter:06d}.jpg\"\n",
    "            cv2.imwrite(f\"{OUTPUT_DIR}/images/{frame_filename}\", resized_frame)\n",
    "\n",
    "            # Create label file only if coordinates are not (0,0)\n",
    "            if x != 0 or y != 0:\n",
    "                # Scale coordinates to target size\n",
    "                x_scaled = x * scale_x\n",
    "                y_scaled = y * scale_y\n",
    "                box_w = DRONE_BOX_SIZE * scale_x\n",
    "                box_h = DRONE_BOX_SIZE * scale_y\n",
    "\n",
    "                # Convert to YOLO format (normalized)\n",
    "                x_center = x_scaled / TARGET_SIZE[0]\n",
    "                y_center = y_scaled / TARGET_SIZE[1]\n",
    "                w = box_w / TARGET_SIZE[0]\n",
    "                h = box_h / TARGET_SIZE[1]\n",
    "\n",
    "                # Write label file\n",
    "                label_filename = f\"{FRAME_NAME_PREFIX}_{global_frame_counter:06d}.txt\"\n",
    "                with open(f\"{OUTPUT_DIR}/labels/{label_filename}\", \"w\") as f:\n",
    "                    f.write(f\"0 {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "\n",
    "            global_frame_counter += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    return global_frame_counter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all video files\n",
    "video_files = [f for f in os.listdir(INPUT_DIR) if f.endswith(\".mp4\")]\n",
    "global_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing cam1.mp4 with cam1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cam1.mp4: 100%|██████████| 4941/4941 [00:18<00:00, 271.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing cam3.mp4 with cam3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cam3.mp4: 100%|██████████| 4080/4080 [00:14<00:00, 279.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing cam2.mp4 with cam2.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cam2.mp4: 100%|██████████| 8016/8016 [00:30<00:00, 264.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing cam6.mp4 with cam6.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cam6.mp4:  26%|██▌       | 4419/17166 [00:54<02:25, 87.79it/s]"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for video_file in video_files:\n",
    "    cam_match = re.search(r'cam(\\d+)', video_file)\n",
    "    if not cam_match:\n",
    "        print(f\"Skipping {video_file} (couldn't extract cam number)\")\n",
    "        continue\n",
    "\n",
    "    cam_number = cam_match.group(1)\n",
    "    txt_file = f\"cam{cam_number}.txt\"\n",
    "\n",
    "    video_path = os.path.join(INPUT_DIR, video_file)\n",
    "    txt_path = os.path.join(INPUT_DIR, txt_file)\n",
    "\n",
    "    if not os.path.exists(txt_path):\n",
    "        print(f\"Skipping {video_file} (missing {txt_file})\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing {video_file} with {txt_file}\")\n",
    "    global_counter = process_video_pair(video_path, txt_path, global_counter)\n",
    "\n",
    "print(\"\\nPreprocessing completed successfully!\")\n",
    "print(f\"Total frames processed: {global_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'processed_dataset/images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[1;32m     83\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 85\u001b[0m plot_samples()\n",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m, in \u001b[0;36mplot_samples\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_samples\u001b[39m():\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Get all image files\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     image_files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/images\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     17\u001b[0m     selected_files \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(image_files, \u001b[38;5;28mmin\u001b[39m(NUM_SAMPLES, \u001b[38;5;28mlen\u001b[39m(image_files)))\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Create one figure per sample with full width\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'processed_dataset/images'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \"processed_dataset\"\n",
    "NUM_SAMPLES = 10\n",
    "PADDING = 10  # Pixels to add around each bounding box\n",
    "IMG_WIDTH = 20  # Inches (full notebook width)\n",
    "IMG_HEIGHT_PER_SAMPLE = 5  # Inches per image\n",
    "\n",
    "def plot_samples():\n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(f\"{DATA_DIR}/images\") if f.endswith(\".jpg\")]\n",
    "    selected_files = random.sample(image_files, min(NUM_SAMPLES, len(image_files)))\n",
    "    \n",
    "    # Create one figure per sample with full width\n",
    "    fig, axes = plt.subplots(\n",
    "        NUM_SAMPLES, 1,\n",
    "        figsize=(IMG_WIDTH, IMG_HEIGHT_PER_SAMPLE * NUM_SAMPLES)\n",
    "    )\n",
    "    \n",
    "    for i, img_file in enumerate(selected_files):\n",
    "        ax = axes[i] if NUM_SAMPLES > 1 else axes\n",
    "        \n",
    "        # Read image\n",
    "        img_path = os.path.join(DATA_DIR, \"images\", img_file)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Display image\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Set title\n",
    "        frame_num = img_file.split('_')[1].split('.')[0]\n",
    "        ax.set_title(f\"Frame {frame_num}\", pad=10)\n",
    "        \n",
    "        # Check for corresponding label\n",
    "        label_path = os.path.join(DATA_DIR, \"labels\", img_file.replace(\".jpg\", \".txt\"))\n",
    "        has_drone = os.path.exists(label_path)\n",
    "        \n",
    "        # Plot bounding box if drone exists\n",
    "        if has_drone:\n",
    "            with open(label_path, 'r') as f:\n",
    "                line = f.readline().strip()\n",
    "                cls, x_center, y_center, w, h = map(float, line.split())\n",
    "                \n",
    "                # Convert from YOLO to pixel coordinates\n",
    "                img_h, img_w = img.shape[:2]\n",
    "                x = (x_center - w/2) * img_w\n",
    "                y = (y_center - h/2) * img_h\n",
    "                width = w * img_w\n",
    "                height = h * img_h\n",
    "                \n",
    "                # Create padded rectangle\n",
    "                rect = plt.Rectangle(\n",
    "                    (x-PADDING, y-PADDING),\n",
    "                    width + 2*PADDING,\n",
    "                    height + 2*PADDING,\n",
    "                    linewidth=2,\n",
    "                    edgecolor='r',\n",
    "                    facecolor='none',\n",
    "                    linestyle='-'\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                # Add center marker\n",
    "                # ax.scatter(\n",
    "                #     [x_center * img_w], [y_center * img_h],\n",
    "                #     color='lime', s=40, marker='+'\n",
    "                # )\n",
    "        else:\n",
    "            ax.text(\n",
    "                img.shape[1]//2, img.shape[0]//2,\n",
    "                \"NO DRONE\", color='white',\n",
    "                ha='center', va='center',\n",
    "                bbox=dict(facecolor='red', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drone frames: 61284 (69.6%)\n",
      "No-drone frames: 26754 (30.4%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \"processed_dataset/labels\"  # Path to your label files\n",
    "\n",
    "# Initialize counters\n",
    "drone_frames = 0\n",
    "no_drone_frames = 0\n",
    "\n",
    "# Count all label files (each represents a drone frame)\n",
    "label_files = [f for f in os.listdir(DATA_DIR) if f.endswith('.txt')]\n",
    "drone_frames = len(label_files)\n",
    "\n",
    "# Count total frames (image files)\n",
    "image_files = [f for f in os.listdir(DATA_DIR.replace('labels', 'images')) if f.endswith('.jpg')]\n",
    "total_frames = len(image_files)\n",
    "no_drone_frames = total_frames - drone_frames\n",
    "\n",
    "# Print results\n",
    "print(f\"Drone frames: {drone_frames} ({drone_frames/total_frames:.1%})\")\n",
    "print(f\"No-drone frames: {no_drone_frames} ({no_drone_frames/total_frames:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 2.0.5 (you have 1.4.11). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Device configuration for Apple Silicon\n",
    "DEVICE = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"processed_dataset\"\n",
    "BATCH_SIZE = 8  # Reduced for MPS memory constraints\n",
    "NUM_EPOCHS = 50\n",
    "IMG_SIZE = (640, 360)\n",
    "NUM_WORKERS = 0 if DEVICE.type == 'mps' else 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_drone_aug = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.Rotate(limit=30, p=0.4),\n",
    "    A.GaussianBlur(blur_limit=(3, 7), p=0.2),\n",
    "    A.CLAHE(p=0.3),\n",
    "    A.ChannelShuffle(p=0.1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroneDataset(Dataset):\n",
    "    def __init__(self, image_paths, label_paths, augment=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.augment = augment\n",
    "        self.class_weights = [0.7, 1.3]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label_path = self.label_paths[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Load targets\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    cls, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "                    \n",
    "                    # Convert to xyxy format\n",
    "                    x_min = (x_center - width/2) * IMG_SIZE[0]\n",
    "                    y_min = (y_center - height/2) * IMG_SIZE[1]\n",
    "                    x_max = x_min + width * IMG_SIZE[0]\n",
    "                    y_max = y_min + height * IMG_SIZE[1]\n",
    "                    \n",
    "                    boxes.append([x_min, y_min, x_max, y_max])\n",
    "                    labels.append(int(cls))\n",
    "        \n",
    "        # Apply augmentations for no-drone images\n",
    "        if self.augment and len(boxes) == 0:\n",
    "            aug = no_drone_aug(image=image)\n",
    "            image = aug['image']\n",
    "\n",
    "        # Convert to tensors with proper dimensionality\n",
    "        image = F.to_tensor(image)\n",
    "        \n",
    "        # Handle empty boxes case\n",
    "        if len(boxes) == 0:\n",
    "            boxes_tensor = torch.zeros((0, 4), dtype=torch.float32)\n",
    "        else:\n",
    "            boxes_tensor = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        \n",
    "        target = {\n",
    "            'boxes': boxes_tensor,\n",
    "            'labels': torch.as_tensor(labels, dtype=torch.int64),\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'area': (boxes_tensor[:, 3] - boxes_tensor[:, 1]) * \n",
    "                    (boxes_tensor[:, 2] - boxes_tensor[:, 0]),\n",
    "            'iscrowd': torch.zeros(len(boxes), dtype=torch.int64)\n",
    "        }\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% [markdown]\n",
    "### Step 3: Prepare Dataset with PROPER Label Handling\n",
    "# Get all image/label pairs with explicit pairing\n",
    "# Get all image/label pairs with explicit pairing\n",
    "image_label_pairs = [\n",
    "    (\n",
    "        os.path.join(DATA_DIR, 'images', f), \n",
    "        os.path.join(DATA_DIR, 'labels', f.replace('.jpg', '.txt'))\n",
    "    ) for f in os.listdir(os.path.join(DATA_DIR, 'images'))\n",
    "]\n",
    "\n",
    "# Split dataset with paired items\n",
    "train_pairs, val_pairs = train_test_split(\n",
    "    image_label_pairs, \n",
    "    test_size=0.2, \n",
    "    stratify=[int(os.path.exists(lbl)) for img, lbl in image_label_pairs]\n",
    ")\n",
    "\n",
    "# Separate drone/no-drone PAIRS\n",
    "train_drone_pairs = [(img, lbl) for img, lbl in train_pairs if os.path.exists(lbl)]\n",
    "train_no_drone_pairs = [(img, lbl) for img, lbl in train_pairs if not os.path.exists(lbl)]\n",
    "\n",
    "# Oversample no-drone PAIRS (images + labels together)\n",
    "num_to_generate = max(0, len(train_drone_pairs) - len(train_no_drone_pairs))\n",
    "\n",
    "if num_to_generate > 0 and len(train_no_drone_pairs) > 0:\n",
    "    new_indices = np.random.choice(range(len(train_no_drone_pairs)), num_to_generate, replace=True)\n",
    "    train_no_drone_pairs += [train_no_drone_pairs[i] for i in new_indices]\n",
    "\n",
    "# Create balanced dataset with CORRECT PAIRS\n",
    "balanced_pairs = train_drone_pairs + train_no_drone_pairs\n",
    "\n",
    "# Separate into images and labels lists\n",
    "balanced_train_images = [img for img, lbl in balanced_pairs]\n",
    "balanced_train_labels = [lbl for img, lbl in balanced_pairs]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DroneDataset(balanced_train_images, balanced_train_labels, augment=True)\n",
    "val_dataset = DroneDataset([img for img, lbl in val_pairs], [lbl for img, lbl in val_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check pairing consistency\n",
    "for img, lbl in zip(balanced_train_images, balanced_train_labels):\n",
    "    assert os.path.exists(lbl) == os.path.exists(lbl), \"Mismatched pair!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(dataset, num_samples=10):\n",
    "    # Select random indices\n",
    "    indices = random.sample(range(len(dataset)), num_samples)\n",
    "    \n",
    "    # Create one row per sample with full width\n",
    "    fig, axes = plt.subplots(num_samples, 1, figsize=(20, 5*num_samples))\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        image, target = dataset[idx]\n",
    "        ax = axes[i] if num_samples > 1 else axes\n",
    "        \n",
    "        # Convert tensor to numpy array\n",
    "        img = image.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Display image\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Get filename\n",
    "        img_path = dataset.image_paths[idx]\n",
    "        frame_num = os.path.basename(img_path).split('_')[1].split('.')[0]\n",
    "        ax.set_title(f\"Frame {frame_num}\", pad=10)\n",
    "        \n",
    "        # Draw bounding boxes if present\n",
    "        if len(target['boxes']) > 0:\n",
    "            for box in target['boxes']:\n",
    "                xmin, ymin, xmax, ymax = box.numpy()\n",
    "                \n",
    "                # Create rectangle with padding\n",
    "                padding = 5\n",
    "                rect = plt.Rectangle(\n",
    "                    (xmin-padding, ymin-padding),\n",
    "                    (xmax-xmin)+2*padding,\n",
    "                    (ymax-ymin)+2*padding,\n",
    "                    linewidth=2,\n",
    "                    edgecolor='r',\n",
    "                    facecolor='none'\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                # Mark center\n",
    "                # ax.scatter(\n",
    "                #     [(xmin + xmax)/2], [(ymin + ymax)/2],\n",
    "                #     color='lime', s=40, marker='+'\n",
    "                # )\n",
    "        else:\n",
    "            ax.text(\n",
    "                img.shape[1]//2, img.shape[0]//2,\n",
    "                \"NO DRONE\", color='white',\n",
    "                ha='center', va='center',\n",
    "                bbox=dict(facecolor='red', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize samples (will now scroll vertically)\n",
    "visualize_samples(train_dataset, num_samples=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F  # Add this import at the top\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"processed_dataset\"\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 50\n",
    "IMG_SIZE = (640, 360)\n",
    "DEVICE = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: Dynamic Bounding Box Handling\n",
    "\n",
    "class DroneDataset(Dataset):\n",
    "    def __init__(self, image_paths, label_paths, augment=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Calculate dynamic box statistics\n",
    "        self.box_sizes = self._calculate_box_stats()\n",
    "        self.mean_size = np.mean(self.box_sizes) if self.box_sizes else 30\n",
    "        \n",
    "    def _calculate_box_stats(self):\n",
    "        sizes = []\n",
    "        for lbl_path in self.label_paths:\n",
    "            if os.path.exists(lbl_path):\n",
    "                with open(lbl_path, 'r') as f:\n",
    "                    line = f.readline().strip()\n",
    "                    if line:\n",
    "                        _, _, _, w, h = map(float, line.split())\n",
    "                        sizes.append((w*IMG_SIZE[0], h*IMG_SIZE[1]))\n",
    "        return sizes\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label_path = self.label_paths[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    cls, x_center, y_center, w, h = map(float, line.strip().split())\n",
    "                    \n",
    "                    # Dynamic box scaling based on dataset statistics\n",
    "                    scale_factor = 1 + random.uniform(-0.3, 0.3)  # ±30% variation\n",
    "                    w *= scale_factor\n",
    "                    h *= scale_factor\n",
    "                    \n",
    "                    # Convert to xyxy format\n",
    "                    x_min = (x_center - w/2) * IMG_SIZE[0]\n",
    "                    y_min = (y_center - h/2) * IMG_SIZE[1]\n",
    "                    x_max = x_min + w * IMG_SIZE[0]\n",
    "                    y_max = y_min + h * IMG_SIZE[1]\n",
    "                    \n",
    "                    boxes.append([x_min, y_min, x_max, y_max])\n",
    "                    labels.append(int(cls))\n",
    "        \n",
    "        # Convert to tensors\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "        \n",
    "        target = {\n",
    "            'boxes': torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4), dtype=torch.float32),\n",
    "            'labels': torch.tensor(labels, dtype=torch.int64),\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'area': (torch.tensor(boxes)[:, 3] - torch.tensor(boxes)[:, 1]) * \n",
    "                    (torch.tensor(boxes)[:, 2] - torch.tensor(boxes)[:, 0]) if boxes else torch.zeros(0),\n",
    "            'iscrowd': torch.zeros(len(boxes), dtype=torch.int64) if boxes else torch.zeros(0)\n",
    "        }\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image-label pairs\n",
    "image_label_pairs = []\n",
    "for img_file in os.listdir(f\"{DATA_DIR}/images\"):\n",
    "    if img_file.endswith(\".jpg\"):\n",
    "        img_path = os.path.join(DATA_DIR, \"images\", img_file)\n",
    "        label_path = os.path.join(DATA_DIR, \"labels\", img_file.replace(\".jpg\", \".txt\"))\n",
    "        image_label_pairs.append((img_path, label_path))\n",
    "\n",
    "# Split dataset\n",
    "train_pairs, val_pairs = train_test_split(\n",
    "    image_label_pairs,\n",
    "    test_size=0.2,\n",
    "    stratify=[int(os.path.exists(pair[1])) for pair in image_label_pairs],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_drone = [pair for pair in train_pairs if os.path.exists(pair[1])]\n",
    "train_no_drone = [pair for pair in train_pairs if not os.path.exists(pair[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic oversampling based on imbalance ratio\n",
    "oversample_ratio = max(1, len(train_drone) // max(len(train_no_drone), 1))\n",
    "train_no_drone = train_no_drone * oversample_ratio\n",
    "random.shuffle(train_no_drone)\n",
    "\n",
    "balanced_train = train_drone + train_no_drone[:len(train_drone)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = DroneDataset(\n",
    "    [pair[0] for pair in balanced_train],\n",
    "    [pair[1] for pair in balanced_train],\n",
    "    augment=True\n",
    ")\n",
    "val_dataset = DroneDataset(\n",
    "    [pair[0] for pair in val_pairs],\n",
    "    [pair[1] for pair in val_pairs]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Balance:\n",
      "Total samples: 91833\n",
      "Drone samples: 49027 (53.4%)\n",
      "No drone samples: 42806 (46.6%)\n",
      "Status:  ✅ Balanced\n",
      "\n",
      "Validation Set Balance:\n",
      "Total samples: 17608\n",
      "Drone samples: 12257 (69.6%)\n",
      "No drone samples: 5351 (30.4%)\n",
      "Status:  ❌ Imbalanced\n"
     ]
    }
   ],
   "source": [
    "def check_balance(dataset, name=\"Dataset\"):\n",
    "    drone_count = 0\n",
    "    no_drone_count = 0\n",
    "    \n",
    "    for img_path, lbl_path in zip(dataset.image_paths, dataset.label_paths):\n",
    "        if os.path.exists(lbl_path):\n",
    "            drone_count += 1\n",
    "        else:\n",
    "            no_drone_count += 1\n",
    "    \n",
    "    total = drone_count + no_drone_count\n",
    "    print(f\"\\n{name} Balance:\")\n",
    "    print(f\"Total samples: {total}\")\n",
    "    print(f\"Drone samples: {drone_count} ({drone_count/total:.1%})\")\n",
    "    print(f\"No drone samples: {no_drone_count} ({no_drone_count/total:.1%})\")\n",
    "    print(\"Status: \", \"✅ Balanced\" if abs(drone_count - no_drone_count)/total < 0.1 else \"❌ Imbalanced\")\n",
    "\n",
    "# Check training set balance\n",
    "check_balance(train_dataset, \"Training Set\")\n",
    "\n",
    "# Check validation set balance\n",
    "check_balance(val_dataset, \"Validation Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_fastrcnn_loss(class_logits, box_regression, labels, regression_targets):\n",
    "    # Convert labels list to tensor\n",
    "    labels = torch.cat(labels, dim=0) if isinstance(labels, list) else labels\n",
    "    \n",
    "    # Classification loss\n",
    "    classification_loss = F.cross_entropy(class_logits, labels)\n",
    "\n",
    "    # Box regression loss with MPS workaround\n",
    "    sampled_pos_inds_subset = torch.where(labels > 0)[0]\n",
    "    labels_pos = labels[sampled_pos_inds_subset]\n",
    "    \n",
    "    if labels_pos.numel() == 0:\n",
    "        box_loss = torch.tensor(0.0, device=class_logits.device)\n",
    "    else:\n",
    "        # Ensure regression targets are concatenated\n",
    "        regression_targets = torch.cat(regression_targets, dim=0)\n",
    "        box_regression = box_regression.reshape(-1, box_regression.size(-1) // 4, 4)\n",
    "        \n",
    "        box_loss = F.smooth_l1_loss(\n",
    "            box_regression[sampled_pos_inds_subset, labels_pos],\n",
    "            regression_targets[sampled_pos_inds_subset],\n",
    "            beta=1/9,\n",
    "            reduction=\"sum\"\n",
    "        ) / max(1, labels.numel())\n",
    "\n",
    "    return classification_loss, box_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.roi_heads import RoIHeads\n",
    "\n",
    "class CustomRoIHeads(RoIHeads):\n",
    "    def forward(self, features, proposals, image_shapes, targets=None):\n",
    "        # Original forward pass implementation\n",
    "        if targets is not None:\n",
    "            for t in targets:\n",
    "                floating_point_types = (torch.float, torch.double, torch.half)\n",
    "                assert t[\"boxes\"].dtype in floating_point_types\n",
    "                assert t[\"labels\"].dtype == torch.int64\n",
    "        \n",
    "        if self.training:\n",
    "            proposals, matched_idxs, labels, regression_targets = self.select_training_samples(proposals, targets)\n",
    "        else:\n",
    "            labels = None\n",
    "            regression_targets = None\n",
    "            matched_idxs = None\n",
    "\n",
    "        # Box head forward pass\n",
    "        box_features = self.box_roi_pool(features, proposals, image_shapes)\n",
    "        box_features = self.box_head(box_features)\n",
    "        class_logits, box_regression = self.box_predictor(box_features)\n",
    "\n",
    "        # Calculate losses\n",
    "        if self.training:\n",
    "            loss_classifier, loss_box_reg = custom_fastrcnn_loss(\n",
    "                class_logits, \n",
    "                box_regression, \n",
    "                labels, \n",
    "                regression_targets\n",
    "            )\n",
    "            losses = {\n",
    "                \"loss_classifier\": loss_classifier,\n",
    "                \"loss_box_reg\": loss_box_reg\n",
    "            }\n",
    "            return proposals, losses\n",
    "        else:\n",
    "            boxes, scores, labels = self.postprocess_detections(class_logits, box_regression, proposals, image_shapes)\n",
    "            return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// ... existing code ...\n",
    "def create_model():\n",
    "    # Check for cached weights\n",
    "    weights_path = os.path.expanduser('~/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth')\n",
    "    if os.path.exists(weights_path):\n",
    "        print(\"Using cached ResNet50 weights\")\n",
    "        weights = torch.load(weights_path, map_location=DEVICE)\n",
    "    else:\n",
    "        print(\"Downloading ResNet50 weights...\")\n",
    "        weights = None\n",
    "\n",
    "    # Anchor configuration\n",
    "    anchor_sizes = ((8,), (16,), (32,), (64,), (128,))\n",
    "    aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "\n",
    "    print(\"Creating model architecture...\")\n",
    "    # Create base model\n",
    "    model = fasterrcnn_resnet50_fpn(\n",
    "        weights=weights,\n",
    "        min_size=IMG_SIZE[1],\n",
    "        max_size=IMG_SIZE[0],\n",
    "        rpn_anchor_generator=AnchorGenerator(anchor_sizes, aspect_ratios),\n",
    "        box_roi_pool=MultiScaleRoIAlign(\n",
    "            featmap_names=['0', '1', '2', '3', 'pool'],\n",
    "            output_size=7,\n",
    "            sampling_ratio=2\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"Configuring custom ROI heads...\")\n",
    "    # Configure custom ROI heads with explicit parameters\n",
    "    model.roi_heads = CustomRoIHeads(\n",
    "        box_roi_pool=model.roi_heads.box_roi_pool,\n",
    "        box_head=model.roi_heads.box_head,\n",
    "        box_predictor=model.roi_heads.box_predictor,\n",
    "        fg_iou_thresh=0.5,\n",
    "        bg_iou_thresh=0.5,\n",
    "        batch_size_per_image=512,\n",
    "        positive_fraction=0.25,\n",
    "        bbox_reg_weights=None,\n",
    "        score_thresh=0.05,\n",
    "        nms_thresh=0.5,\n",
    "        detections_per_img=100\n",
    "    )\n",
    "\n",
    "    print(\"Initializing model weights...\")\n",
    "    # Initialize weights\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            torch.nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    print(\"Moving model to device...\")\n",
    "    model = model.to(DEVICE)\n",
    "    print(\"Model initialization complete!\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// ... existing code ...\n",
    "def train_model():\n",
    "    print(\"Starting model initialization...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = create_model()\n",
    "    print(f\"Model initialization took {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.0005)\n",
    "    \n",
    "    print(\"Preparing data loaders...\")\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "    # Create weighted sampler\n",
    "    weights = [2.0 if os.path.exists(lbl) else 1.0 for (img, lbl) in balanced_train]\n",
    "    sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=sampler,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0  # Required for MPS\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_start = time.time()\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "            images = [img.to(DEVICE) for img in images]\n",
    "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += losses.item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:  # Print progress every 10 batches\n",
    "                print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Batch {batch_idx}/{len(train_loader)} | Loss: {losses.item():.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images = [img.to(DEVICE) for img in images]\n",
    "                targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "                try:\n",
    "                    loss_dict = model(images, targets)\n",
    "                    val_loss += sum(loss for loss in loss_dict.values()).item()\n",
    "                    val_batches += 1\n",
    "                except ValueError:\n",
    "                    # Skip this batch if there's a value error due to ROI heads mismatch\n",
    "                    print(\"Skipping validation batch due to ROI heads mismatch\")\n",
    "                    continue\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {total_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f} | Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    print(\"Saving model...\")\n",
    "    torch.save(model.state_dict(), 'drone_model.pth')\n",
    "    print(\"Training complete!\")\n",
    "    return model\n",
    "// ... existing code ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--visualize tracking results and logs: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% [markdown]\n",
    "### Step 4: Visualization and Analysis of Tracking Results\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_tracking_results(results_dir=\"results\"):\n",
    "    # Find all result files\n",
    "    import glob\n",
    "    metric_files = glob.glob(f\"{results_dir}/*_metrics.json\")\n",
    "    \n",
    "    if not metric_files:\n",
    "        print(\"No results found in the results directory\")\n",
    "        return\n",
    "    \n",
    "    # Create visualization directory\n",
    "    os.makedirs(f\"{results_dir}/visualizations\", exist_ok=True)\n",
    "    \n",
    "    # Process each result file\n",
    "    for metric_file in metric_files:\n",
    "        base_name = metric_file.replace(\"_metrics.json\", \"\")\n",
    "        video_name = base_name.split(\"/\")[-1].split(\"_\")[0]\n",
    "        \n",
    "        print(f\"\\nVisualizing results for {video_name}\")\n",
    "        \n",
    "        # Load metrics\n",
    "        with open(metric_file) as f:\n",
    "            metrics = json.load(f)\n",
    "        \n",
    "        # Load visibility log\n",
    "        visibility_file = f\"{base_name}_visibility.log\"\n",
    "        visibility_data = []\n",
    "        current_id = None\n",
    "        with open(visibility_file) as f:\n",
    "            for line in f:\n",
    "                if \"Drone ID\" in line:\n",
    "                    if \"Visible from\" in line:\n",
    "                        parts = line.strip().split()\n",
    "                        start = datetime.strptime(\" \".join(parts[3:5]), \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "                        end = datetime.strptime(\" \".join(parts[7:9]), \"%Y-%m-%d %H:%M:%S.%f\") if parts[7] != \"present\" else None\n",
    "                        duration = (end - start).total_seconds() if end else None\n",
    "                        visibility_data.append({\n",
    "                            'drone_id': current_id,\n",
    "                            'start': start,\n",
    "                            'end': end,\n",
    "                            'duration': duration\n",
    "                        })\n",
    "                    else:\n",
    "                        current_id = int(line.split()[2].replace(\":\", \"\"))\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(visibility_data)\n",
    "        \n",
    "        # Convert timestamps to relative seconds\n",
    "        min_time = df['start'].min()\n",
    "        df['start_rel'] = df['start'].apply(lambda x: (x - min_time).total_seconds())\n",
    "        df['end_rel'] = df.apply(lambda row: (row['end'] - min_time).total_seconds() \n",
    "                                if row['end'] else (datetime.now() - min_time).total_seconds(), axis=1)\n",
    "        \n",
    "        # Plot 1: Visibility Timeline\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        for _, row in df.iterrows():\n",
    "            plt.plot([row['start_rel'], row['end_rel']], [row['drone_id'], row['drone_id']], \n",
    "                    linewidth=5, marker=\"|\", markersize=10)\n",
    "        plt.title(f\"Drone Visibility Timeline - {video_name}\")\n",
    "        plt.xlabel(\"Time (seconds)\")\n",
    "        plt.ylabel(\"Drone ID\")\n",
    "        plt.yticks(df['drone_id'].unique())\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f\"{results_dir}/visualizations/{video_name}_timeline.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot 2: Visibility Duration\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        duration_df = df.groupby('drone_id')['duration'].sum().reset_index()\n",
    "        sns.barplot(data=duration_df, x='drone_id', y='duration')\n",
    "        plt.title(f\"Total Visibility Time by Drone - {video_name}\")\n",
    "        plt.xlabel(\"Drone ID\")\n",
    "        plt.ylabel(\"Total Visibility (seconds)\")\n",
    "        plt.savefig(f\"{results_dir}/visualizations/{video_name}_duration.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot 3: Reappearances\n",
    "        reappearances = df.groupby('drone_id').size() - 1\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=reappearances.index, y=reappearances.values)\n",
    "        plt.title(f\"Number of Reappearances by Drone - {video_name}\")\n",
    "        plt.xlabel(\"Drone ID\")\n",
    "        plt.ylabel(\"Number of Reappearances\")\n",
    "        plt.savefig(f\"{results_dir}/visualizations/{video_name}_reappearances.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Display metrics\n",
    "        print(\"\\nPerformance Metrics:\")\n",
    "        print(f\"Processing Time: {metrics['processing_time']:.2f} seconds\")\n",
    "        print(f\"Average FPS: {metrics['avg_fps']:.2f}\")\n",
    "        print(f\"Redetection Count: {metrics['redetection_count']}\")\n",
    "        print(f\"Average Detection Time: {metrics['avg_detection_time']:.4f} seconds\")\n",
    "        print(f\"Average Tracking Time: {metrics['avg_tracking_time']:.4f} seconds\")\n",
    "        \n",
    "        # Save combined report\n",
    "        report = {\n",
    "            'video': metrics['video'],\n",
    "            'processing_time': metrics['processing_time'],\n",
    "            'avg_fps': metrics['avg_fps'],\n",
    "            'redetection_count': metrics['redetection_count'],\n",
    "            'drones_tracked': len(df['drone_id'].unique()),\n",
    "            'total_visibility_time': duration_df['duration'].sum(),\n",
    "            'avg_detection_time': metrics['avg_detection_time'],\n",
    "            'avg_tracking_time': metrics['avg_tracking_time']\n",
    "        }\n",
    "        \n",
    "        with open(f\"{results_dir}/visualizations/{video_name}_report.json\", 'w') as f:\n",
    "            json.dump(report, f, indent=4)\n",
    "        \n",
    "        print(f\"\\nVisualizations saved to {results_dir}/visualizations/\")\n",
    "\n",
    "# Run visualization\n",
    "visualize_tracking_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Enhanced Visualization Cell -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% [markdown]\n",
    "### Step 4: Enhanced Tracking Visualization with Flight Paths\n",
    "\n",
    "def visualize_flight_paths(results_dir=\"results\"):\n",
    "    # Find all tracking videos\n",
    "    import glob\n",
    "    video_files = glob.glob(f\"{results_dir}/*_tracked.mp4\")\n",
    "    \n",
    "    if not video_files:\n",
    "        print(\"No tracked videos found in the results directory\")\n",
    "        return\n",
    "    \n",
    "    for video_file in video_files:\n",
    "        video_name = os.path.basename(video_file).replace(\"_tracked.mp4\", \"\")\n",
    "        print(f\"\\nVisualizing flight paths for {video_name}\")\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(f\"{results_dir}/flight_paths\", exist_ok=True)\n",
    "        \n",
    "        # Load corresponding metrics file\n",
    "        metrics_file = f\"{results_dir}/{video_name}_metrics.json\"\n",
    "        with open(metrics_file) as f:\n",
    "            metrics = json.load(f)\n",
    "        \n",
    "        # Extract trajectory data\n",
    "        trajectories = {}\n",
    "        for drone_id, data in metrics['visibility_stats'].items():\n",
    "            trajectories[drone_id] = {\n",
    "                'positions': [],\n",
    "                'color': plt.cm.tab10(int(drone_id) % 10)\n",
    "            }\n",
    "        \n",
    "        # Read video to get flight paths\n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        frame_count = 0\n",
    "        \n",
    "        # We'll sample every nth frame to reduce processing\n",
    "        sample_rate = 10  \n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            if frame_count % sample_rate == 0:\n",
    "                # Convert to RGB for visualization\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Find all drone markers in the frame\n",
    "                for drone_id in trajectories.keys():\n",
    "                    # Look for drone ID text in the frame\n",
    "                    text = f\"ID {drone_id}\"\n",
    "                    result = cv2.matchTemplate(\n",
    "                        frame_rgb, \n",
    "                        np.zeros((30, 100, 3), dtype=np.uint8),  # Dummy template\n",
    "                        cv2.TM_CCOEFF_NORMED\n",
    "                    )\n",
    "                    # (In practice, you'd need a better way to extract positions)\n",
    "                    # This is simplified - actual implementation would track positions\n",
    "                    \n",
    "            frame_count += 1\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        # Alternative: Use the trajectory data from the tracker\n",
    "        # (This assumes you saved the trajectory data in metrics)\n",
    "        if 'trajectory_data' in metrics:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            for drone_id, path in metrics['trajectory_data'].items():\n",
    "                if len(path) > 1:\n",
    "                    x = [p[0] for p in path]\n",
    "                    y = [p[1] for p in path]\n",
    "                    color = plt.cm.tab10(int(drone_id) % 10)\n",
    "                    \n",
    "                    # Plot the flight path\n",
    "                    plt.plot(x, y, '.-', color=color, label=f'Drone {drone_id}', \n",
    "                            alpha=0.6, linewidth=2, markersize=8)\n",
    "                    \n",
    "                    # Add start and end markers\n",
    "                    plt.scatter(x[0], y[0], color='green', s=100, marker='o', edgecolors='white')\n",
    "                    plt.scatter(x[-1], y[-1], color='red', s=100, marker='X', edgecolors='white')\n",
    "            \n",
    "            plt.title(f\"Drone Flight Paths - {video_name}\")\n",
    "            plt.xlabel(\"X Position (pixels)\")\n",
    "            plt.ylabel(\"Y Position (pixels)\")\n",
    "            plt.gca().invert_yaxis()  # Match image coordinate system\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            \n",
    "            # Add frame reference\n",
    "            if 'frame_size' in metrics:\n",
    "                w, h = metrics['frame_size']\n",
    "                plt.gca().add_patch(Rectangle((0, 0), w, h, fill=False, edgecolor='gray', linestyle='--'))\n",
    "            \n",
    "            plt.savefig(f\"{results_dir}/flight_paths/{video_name}_paths.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"Flight path visualization saved to {results_dir}/flight_paths/{video_name}_paths.png\")\n",
    "        else:\n",
    "            print(\"No trajectory data found in metrics\")\n",
    "\n",
    "# First modify your tracking code to save trajectory data\n",
    "def add_trajectory_saving():\n",
    "    # Add this to your DroneDetectionAndTracking class in track_drone.py\n",
    "    def _save_results(self, video_path, start_time, processed_frames):\n",
    "        # ... existing code ...\n",
    "        \n",
    "        # Add trajectory data to metrics\n",
    "        metrics['trajectory_data'] = {\n",
    "            str(k): v for k, v in self.tracker.trajectory.items()\n",
    "        }\n",
    "        metrics['frame_size'] = (int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "                                int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "        \n",
    "        # ... rest of existing code ...\n",
    "\n",
    "# Then run visualizations\n",
    "print(\"Visualizing flight paths...\")\n",
    "visualize_flight_paths()\n",
    "\n",
    "print(\"\\nVisualizing tracking metrics...\")\n",
    "visualize_tracking_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Initialize MPS optimizations\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     model \u001b[38;5;241m=\u001b[39m train_model()\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Cleanup\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DEVICE\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[0;32mIn[27], line 36\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m images \u001b[38;5;241m=\u001b[39m [img\u001b[38;5;241m.\u001b[39mto(DEVICE) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m     34\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(DEVICE) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[0;32m---> 36\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m model(images, targets)\n\u001b[1;32m     37\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/venv-metal/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/venv-metal/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/venv-metal/lib/python3.11/site-packages/torchvision/models/detection/generalized_rcnn.py:83\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     77\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_assert(\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28mlen\u001b[39m(val) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting the last two dimensions of the Tensor to be H and W instead got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     80\u001b[0m     )\n\u001b[1;32m     81\u001b[0m     original_image_sizes\u001b[38;5;241m.\u001b[39mappend((val[\u001b[38;5;241m0\u001b[39m], val[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m---> 83\u001b[0m images, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(images, targets)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Check for degenerate boxes\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# TODO: Move this to a function\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/venv-metal/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/venv-metal/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/venv-metal/lib/python3.11/site-packages/torchvision/models/detection/transform.py:142\u001b[0m, in \u001b[0;36mGeneralizedRCNNTransform.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages is expected to be a list of 3d tensors of shape [C, H, W], got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    141\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(image)\n\u001b[0;32m--> 142\u001b[0m image, target_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize(image, target_index)\n\u001b[1;32m    143\u001b[0m images[i] \u001b[38;5;241m=\u001b[39m image\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m target_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/venv-metal/lib/python3.11/site-packages/torchvision/models/detection/transform.py:191\u001b[0m, in \u001b[0;36mGeneralizedRCNNTransform.resize\u001b[0;34m(self, image, target)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_size[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 191\u001b[0m image, target \u001b[38;5;241m=\u001b[39m _resize_image_and_masks(image, size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_size, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_size)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image, target\n",
      "File \u001b[0;32m~/anaconda3/envs/venv-metal/lib/python3.11/site-packages/torchvision/models/detection/transform.py:65\u001b[0m, in \u001b[0;36m_resize_image_and_masks\u001b[0;34m(image, self_min_size, self_max_size, target, fixed_size)\u001b[0m\n\u001b[1;32m     61\u001b[0m         scale_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(self_min_size \u001b[38;5;241m/\u001b[39m min_size, self_max_size \u001b[38;5;241m/\u001b[39m max_size)\n\u001b[1;32m     63\u001b[0m     recompute_scale_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39minterpolate(\n\u001b[1;32m     66\u001b[0m     image[\u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[1;32m     67\u001b[0m     size\u001b[38;5;241m=\u001b[39msize,\n\u001b[1;32m     68\u001b[0m     scale_factor\u001b[38;5;241m=\u001b[39mscale_factor,\n\u001b[1;32m     69\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     70\u001b[0m     recompute_scale_factor\u001b[38;5;241m=\u001b[39mrecompute_scale_factor,\n\u001b[1;32m     71\u001b[0m     align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image, target\n",
      "File \u001b[0;32m~/anaconda3/envs/venv-metal/lib/python3.11/site-packages/torch/nn/functional.py:4065\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4059\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mare_deterministic_algorithms_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mis_cuda:\n\u001b[1;32m   4060\u001b[0m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put\u001b[39;00m\n\u001b[1;32m   4061\u001b[0m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[1;32m   4062\u001b[0m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[1;32m   4063\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch._decomp.decompositions\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39m_upsample_linear_vec(\n\u001b[1;32m   4064\u001b[0m                 \u001b[38;5;28minput\u001b[39m, output_size, align_corners, scale_factors)\n\u001b[0;32m-> 4065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mupsample_bilinear2d(\u001b[38;5;28minput\u001b[39m, output_size, align_corners, scale_factors)\n\u001b[1;32m   4066\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   4067\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = train_model()\n",
    "    \n",
    "    # Cleanup\n",
    "    if DEVICE.type == 'mps':\n",
    "        torch.mps.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
